{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Hello Spark!!!\n",
    "Apache Spark is a fast and general-purpose cluster computing system. It provides high-level APIs in Java, Scala, Python and R, and an optimized engine that supports general execution graphs. It also supports a rich set of higher-level tools including Spark SQL for SQL and structured data processing, MLlib for machine learning, GraphX for graph processing, and Spark Streaming.\n",
    "\n",
    "<img src='https://github.com/carloapp2/SparkPOT/blob/master/spark.png?raw=true' width=\"80%\" height=\"80%\"></img>\n",
    "\n",
    "This notebook will show you some basic concepts to start working with Apache Spark including:\n",
    "\n",
    "- Understanding Spark Context\n",
    "- Creating Resilient Distributed Datasets (RDD)\n",
    "- Performing Data Transformations\n",
    "- Loading Data Files to use with Spark\n",
    "\n",
    "####Tool Tips:\n",
    "- Notice the navigation and command buttons at top of the notebook. Press Play & Stop buttons to execute code and interupt execution.\n",
    "- Notice each cell has type. (Markdown, Code, Etc) This cell is a Markdown cell which is simply HTML informational vs Code cell allows you to execute against spark.\n",
    "- Notice each cell has desigination, for eample In [n]: the number is cell number. When you see In [*]: that means the cell is executing\n",
    "- To see all methonds available for object you can use Tab key Example Enter \"SC.\" press Tab and a drop down will appear.\n",
    "- To execute code in active cell press play button at top or you can use short cut keys Shift-Enter, Ctrl-Enter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Spark Driver and Workers programs\n",
    "A Spark program has a driver program and a workers program. Worker programs run on cluster nodes or in local threads. RDDs are distributed\u001d",
    " across workers. \n",
    "\n",
    "<img src='https://github.com/carloapp2/SparkPOT/blob/master/Spark%20Architecture.png?raw=true' width=\"80%\" height=\"80%\"></img>\n",
    "\n",
    "###Python Spark (pySpark)\n",
    "We are using the Python programming interface to Spark pySpark. \n",
    "pySpark provides an easy-to-use programming abstraction and parallel runtime.\n",
    "\n",
    "###Spark Context\n",
    "Apache Spark driver application uses a context allow a programming interface to interact with the driver application. This is know as a Spark Context which supports Python, Scala and Java programming languages. The SparkContext object tells Spark how and where to access a cluster.<br>\n",
    "<font color=\"red\">This lab uses IBM's fully managed cloud based notebook enviornment, so the spark context is predefined for you.</font><br>\n",
    "\n",
    "In other enviornments you would need to pick an interprerter (i.e. pyspark for python) and create a Spark Config object to initilize a Spark Context. <br>\n",
    "\n",
    "Example:<br>\n",
    "from pyspark import SparkContext, SparkConf<br>\n",
    "conf = SparkConf().setAppName(appName).setMaster(master)<br>\n",
    "sc = SparkContext(conf=conf)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 1 - Hello Spark\n",
    "This Lab will show you how to work with Apache Spark using Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 - Working with Spark Context\n",
    "\n",
    "Step 1 - Invoke the spark context and extract what version of the spark driver application.\n",
    "\n",
    "Type<br>\n",
    "sc.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'1.6.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Step 1.1 - Check spark version\n",
    "sc.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Step 2 - Working with Resilient Distributed Datasets\n",
    "\n",
    "Step 2 - Create RDD with numbers 1 to 10,<br>\n",
    "Extract first line,<br>\n",
    "Extract first 5 lines,<br>\n",
    "Create RDD with string \"Hello Spark\",<br>\n",
    "Extract first line.<br>\n",
    "\n",
    "Type: <br>\n",
    "x = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]<br>\n",
    "x_nbr_rdd = sc.parallelize(x)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Step 2.1 - Create RDD of Numbers 1-10\n",
    "x = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "x_nbr_rdd = sc.parallelize(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type: <br>\n",
    "x_nbr_rdd.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Step 2.2 - Extract first line\n",
    "x_nbr_rdd.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type:<br>\n",
    "x_nbr_rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Step 2.3 - Extract first 10 lines\n",
    "x_nbr_rdd.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform a first map transformation and rpelace each element X in the RDD with X+1.<br>\n",
    "Type:<br>\n",
    "x_nbr_rdd_2 = x_nbr_rdd.map(lambda x: x+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n"
     ]
    }
   ],
   "source": [
    "#Step 2.4 - Perform your first map transformation. Replace each element X in the RDD with X+1.\n",
    "#Remember that RDDs are IMMUTABLE, so it is not possible to UPDATE an RDD. You need to create\n",
    "#a NEW RDD\n",
    "x_nbr_rdd_2 = x_nbr_rdd.map(lambda x: x+1)\n",
    "print x_nbr_rdd_2.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the elements of the new RDD.<br>\n",
    "Type:<br>\n",
    "x_nbr_rdd_2.collect()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4, 5, 6, 7, 8, 9, 10, 11]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Step 2.5 - Check out the elements of the new RDD. Warning: Be careful with this in real life !! As you\n",
    "#will be bringing all elements of the RDD (from all partitions) to the driver...\n",
    "x_nbr_rdd_2.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now create a new RDD with one string \"Hello Spark\" and take a look at it.<br>\n",
    "Type:<br>\n",
    "y = [\"Hello Spark!\"]<br>\n",
    "y_str_rdd = sc.parallelize(y)<br>\n",
    "y_str_rdd.first()<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello Spark!'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Step 2.6 - Create String RDD, Extract first line\n",
    "y = [\"Hello Spark!\"]\n",
    "y_str_rdd = sc.parallelize(y)\n",
    "y_str_rdd.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now create a third RDD with several strings.<br>\n",
    "Type:<br>\n",
    "z = [\"First,Line\", \"Second,Line\", \"and,Third,Line\"]<br>\n",
    "z_str_rdd = sc.parallelize(z)<br>\n",
    "z_str_rdd.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'First,Line'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Step 2.7 - Create String RDD with many lines / entries, Extract first line\n",
    "z = [\"First,Line\", \"Second,Line\", \"and,Third,Line\"]\n",
    "z_str_rdd = sc.parallelize(z)\n",
    "z_str_rdd.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the number of entries in this RDD.<br>\n",
    "Type:<br>\n",
    "z_str_rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Step 2.8 - Count the number of entries in the RDD\n",
    "z_str_rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the elements of this RDD.<br>\n",
    "Type:<br>\n",
    "z_str_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['First,Line', 'Second,Line', 'and,Third,Line']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Step 2.9 - Show all the entries in the RDD. Warning: Be careful with this in real life !! \n",
    "#As you will be bringing all elements of the RDD (from all partitions) to the driver...\n",
    "z_str_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step, we will split all the entries in the RDD on the commas \",\" <br>\n",
    "Type: <br>\n",
    "z_str_rdd_split = z_str_rdd.map(lambda line: line.split(\",\"))<br>\n",
    "z_str_rdd_split.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['First', 'Line'], ['Second', 'Line'], ['and', 'Third', 'Line']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Step 2.10 - Perform a map transformation to split all entries in the RDD on the commas \",\".\n",
    "z_str_rdd_split = z_str_rdd.map(lambda line: line.split(\",\"))\n",
    "\n",
    "#Check out the entries in the new RDD\n",
    "z_str_rdd_split.collect()\n",
    "#z_str_rdd_split.count()\n",
    "\n",
    "\n",
    "#Notice how the entries in the new RDD are now ARRAYs with elements, where the original\n",
    "#strings have been split using the comma delimiter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we will learn a new transformation besides map: flatMap <br>\n",
    "flatMap will \"flatten\" all the elements of an RDD entry into its subcomponents<br>\n",
    "This is better explained with an example<br>\n",
    "Type:<br>\n",
    "z_str_rdd_split_flatmap = z_str_rdd.flatMap(lambda line: line.split(\",\")<br>\n",
    "z_str_rdd_split_flatmap.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Step 2.11 - Learn the difference between two transformations: map and flatMap.\n",
    "#Go back to the RDD z_str_rdd_split defined above using a map transformation from z_str_rdd\n",
    "#and use this time a flatmap.\n",
    "z_str_rdd_split_flatmap = z_str_rdd.flatMap(lambda line: line.split(\",\"))\n",
    "z_str_rdd_split_flatmap.collect()\n",
    "z_str_rdd_split_flatmap.count()\n",
    "#What do you notice ? How is z_str_rdd_split_flatmap different from z_str_rdd_split ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we will augment each entry in the previous RDD with the number \"1\" to create pairs (or tuples). The first element of the tuple will be the keyword and the second elements of the tuple will be the digit \"1\".<br>\n",
    "This is a common technic used to count elements using Spark.<br>\n",
    "Type:<br>\n",
    "countWords = z_str_rdd_split_flatmap.map(lambda word:(word,1))<br>\n",
    "countWords.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('First', 1),\n",
       " ('Line', 1),\n",
       " ('Second', 1),\n",
       " ('Line', 1),\n",
       " ('and', 1),\n",
       " ('Third', 1),\n",
       " ('Line', 1)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Step 2.12 - Learn the difference between two transformations: map and flatMap.\n",
    "#Go back to the RDD z_str_rdd_split defined above using a map transformation from z_str_rdd\n",
    "#and use this time a flatmap.\n",
    "\n",
    "countWords = z_str_rdd_split_flatmap.map(lambda word:(word,1))\n",
    "countWords.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have above what is known as a PAIR RDD. Each entry in the RDD has a KEY and a VALUE.<br>\n",
    "The KEY is the word (First, Line, etc...) and the value is the number \"1\"<br>\n",
    "We can now AGGREGATE this RDD by summing up all the values BY KEY<br>\n",
    "Type:<br>\n",
    "from operator import add<br>\n",
    "countWords2 = countWords.reduceByKey(add)<br>\n",
    "countWords2.collect()<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('and', 1), ('Line', 3), ('Second', 1), ('Third', 1), ('First', 1)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Step 2.13 - Check out the results of the aggregation\n",
    "from operator import add\n",
    "countWords2 = countWords.reduceByKey(add)\n",
    "countWords2.collect()\n",
    "\n",
    "#You just created an RDD countWords2 which contains the counts for each token..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Step 3 - Count number of lines with Spark in it\n",
    "Step 3 - Pull in a spark README.md file, <br>\n",
    "Convert the file to an RDD,<br>\n",
    "Count the number of lines with the word \"Spark\" in it. <br>\n",
    "\n",
    "Type:<br>\n",
    "!rm README.md* -f<br>\n",
    "!wget https://raw.githubusercontent.com/brucefischer/Spark_PoT_Federal/master/Modules/README.md<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2016-07-21 12:04:31--  https://raw.githubusercontent.com/brucefischer/Spark_PoT_Federal/master/Modules/README.md\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.48.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.48.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3359 (3.3K) [text/plain]\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "--2016-07-21 12:04:31--  https://raw.githubusercontent.com/brucefischer/Spark_PoT_Federal/master/Modules/README.md\n",
      "Reusing existing connection to raw.githubusercontent.com:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3359 (3.3K) [text/plain]\n",
      "Saving to: 'README.md'\n",
      "\n",
      "100%[======================================>] 3,359       --.-K/s   in 0s      \n",
      "\n",
      "2016-07-21 12:04:31 (27.5 MB/s) - 'README.md' saved [3359/3359]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Step 3.1 - Pull data file into workbench\n",
    "#!rm README.md* -f\n",
    "!wget -N https://raw.githubusercontent.com/brucefischer/Spark_PoT_Federal/master/Modules/README.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will point Spark to the text file stored in the local filesystem and use the \"textFile\" method to create an RDD named \"textfile_rdd\" which will contain one entry for each line in the original text file.<br>\n",
    "We will also count the number of lines in the RDD (which would be as well the number of lines in the text file. <br>\n",
    "Type:<br>\n",
    "textfile_rdd = sc.textFile(\"/resources/README.md\")<br>\n",
    "textfile_rdd.count()<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Step 3.2 - Create RDD from data file\n",
    "textfile_rdd = sc.textFile(\"README.md\")\n",
    "textfile_rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now filter out the RDD and only keep the entries that contain the token \"Spark\". This will be achieved using the \"filter\" transformation, combined with the Python syntax for figuring out whether a particular substring is present within a larger string: substring in string.<br>\n",
    "We will also take a look at the first line in the newly filtered RDD. <br>\n",
    "Type:<br>\n",
    "Spark_lines = textfile_rdd.filter(lambda line: \"Spark\" in line)<br>\n",
    "Spark_lines.first()<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'# Apache Spark'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Step 3.3 - Filter for only lines with word Spark\n",
    "Spark_lines = textfile_rdd.filter(lambda line: \"Spark\" in line)\n",
    "Spark_lines.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now count the number of entries in this filtered RDD and present the result as a concatenated string.<br>\n",
    "Type:<br>\n",
    "print \"The file README.md has \" + str(Spark_lines.count()) + \\<br>\n",
    "\" of \" + str(textfile_rdd.count()) + \\<br>\n",
    "\" Lines with word Spark in it.\"<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file README.md has 17 of 95 Lines with the word Spark in it.\n"
     ]
    }
   ],
   "source": [
    "#Step 3.4 - count the number of lines\n",
    "print \"The file README.md has \" + str(Spark_lines.count()) + \\\n",
    "\" of \" + str(textfile_rdd.count()) + \\\n",
    "\" Lines with the word Spark in it.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using your knowledge from the previous exercises, you will now count the number of times the substring \"Spark\" appears in the original text.<br>\n",
    "Instructions:<br>\n",
    "Looking back at previous exercises, you will need to: <br>\n",
    "1- Execute a flatMap transformation on the original RDD Spark_lines and split on white space.<br>\n",
    "2- Augment each token with the digit \"1\", so as to obtain a PAIR RDD where the first element of the tuple is the token and the second element is the digit \"1\".<br>\n",
    "3- Execute a reduceByKey with the addition to count the number of instances of each token.<br>\n",
    "4- Filter the resulting RDD from Step 3- above to only keep entries which start with \"Spark\".<br> In Python, the syntax to decide whether a string starts with a token is string.startswith(\"token\"). <br>\n",
    "5- Display the resulting list of tokens which start with \"Spark\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'Spark', 13),\n",
       " (u'Spark](#building-spark).', 1),\n",
       " (u'Spark.', 1),\n",
       " (u'SparkPi', 2),\n",
       " (u'Spark\"](http://spark.apache.org/docs/latest/building-spark.html).', 1)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Step 3.5 - count the number of instances of tokens starting with \"Spark\"\n",
    "temp = Spark_lines.flatMap(lambda line:line.split(\" \")).map(lambda x:(x,1)).reduceByKey(add)\n",
    "temp.filter(lambda (k,v): k.startswith(\"Spark\")).collect()\n",
    "#print type(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a slight modification of the cell above, let us now filter out and display the tokens which contain the substring \"Spark\". (Instead of those which only START with it). Your result should be a superset of the previous result. <br>\n",
    "The Python syntax to determine whether a string contains a particular \"token\" is: \"token\" in string<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'Spark', 13),\n",
       " (u'Spark](#building-spark).', 1),\n",
       " (u'Spark.', 1),\n",
       " (u'SparkPi', 2),\n",
       " (u'Spark\"](http://spark.apache.org/docs/latest/building-spark.html).', 1)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Step 3.6 - Display the tokens which contain the substring \"Spark\" in them.\n",
    "temp.filter(lambda (k,v): \"Spark\" in k).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Step 4 - Perform analysis on a data file\n",
    "We have a sample file with instructors and scores. In this exercise we want you to add all scores and report on results by following these steps:<br>\n",
    "\n",
    "1- The name of the file is \"Scores.txt\". Delete it from the local filesystem if it exists.<br>\n",
    "2- Download the file from the provided location (see below).<br>\n",
    "3- Load the text file into an RDD of instructor names and instructor scores.<br>\n",
    "4- Execute a transformation which will keep the instructors names, but will add up the 4 numbers representing the scores per instructor, resulting into a new RDD<br>\n",
    "5- Display the instructor's name and the total score for each instructor<br>\n",
    "6- Execute a second transformation to compute the average score for each instructor and display the results.<br>\n",
    "7- Who was top performer?<br>\n",
    "\n",
    "The Data File has the following format: Instructor Name,Score1,Score2,Score3,Score4<br>\n",
    "Here is an example line from the text file: \"Carlo,5,3,3,4\"<br>\n",
    "Data File Location: https://raw.githubusercontent.com/brucefischer/Spark_PoT_Federal/master/Scores.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2016-07-21 12:04:33--  https://raw.githubusercontent.com/brucefischer/Spark_PoT_Federal/master/Scores.txt\r\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.48.133\r\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.48.133|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 114 [text/plain]\r\n",
      "Last-modified header missing -- time-stamps turned off.\r\n",
      "--2016-07-21 12:04:33--  https://raw.githubusercontent.com/brucefischer/Spark_PoT_Federal/master/Scores.txt\r\n",
      "Reusing existing connection to raw.githubusercontent.com:443.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 114 [text/plain]\r\n",
      "Saving to: 'Scores.txt'\r\n",
      "\r\n",
      "100%[======================================>] 114         --.-K/s   in 0s      \r\n",
      "\r\n",
      "2016-07-21 12:04:33 (36.1 MB/s) - 'Scores.txt' saved [114/114]\r\n",
      "\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[u'Jim,5,3,3,4',\n",
       " u'Hank,2,5,2,2',\n",
       " u'Paul,4,2,4,5',\n",
       " u'Jill,5,2,4,1',\n",
       " u'Fred,5,4,5,1',\n",
       " u'Joan,5,2,2,3',\n",
       " u'Sue,4,2,5,5',\n",
       " u'Barb,5,4,5,1',\n",
       " u'John,5,4,5,1']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Step 4.1 - Delete the file if it exists, download a new copy and load it into an RDD\n",
    "#!rm Scores.txt* -f\n",
    "!wget -N https://raw.githubusercontent.com/brucefischer/Spark_PoT_Federal/master/Scores.txt\n",
    "#!wget https://raw.githubusercontent.com/carloapp2/SparkPOT/master/Scores.txt\n",
    "\n",
    "Raw_Rdd = sc.textFile(\"Scores.txt\")\n",
    "Raw_Rdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'Jim', 15),\n",
       " (u'Hank', 11),\n",
       " (u'Paul', 15),\n",
       " (u'Jill', 12),\n",
       " (u'Fred', 15),\n",
       " (u'Joan', 12),\n",
       " (u'Sue', 16),\n",
       " (u'Barb', 15),\n",
       " (u'John', 15)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Step 4.2 - Execute the necessary transformation(s) to extract the instructor's name, as well\n",
    "# as the instructors scores, then add up the scores per instructor and display the results\n",
    "# in the form of a new RDD with the elements: \"Instructor Name\", InstructorTotals\n",
    "SumScores = Raw_Rdd.map(lambda l: l.split(\",\")).\\\n",
    "map(lambda v : (v[0], int(v[1])+int(v[2])+int(v[3])+int(v[4])))\n",
    "SumScores.take(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'Jim', 15, 3.75),\n",
       " (u'Hank', 11, 2.75),\n",
       " (u'Paul', 15, 3.75),\n",
       " (u'Jill', 12, 3.0),\n",
       " (u'Fred', 15, 3.75),\n",
       " (u'Joan', 12, 3.0),\n",
       " (u'Sue', 16, 4.0),\n",
       " (u'Barb', 15, 3.75),\n",
       " (u'John', 15, 3.75)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Step 4.3 - Execute additional transformation(s) to compute the average score per instructor.\n",
    "# Display the resulting averages for all instructors.\n",
    "from __future__ import division\n",
    "Final = SumScores.map(lambda avg: (avg[0],avg[1],avg[1]/4))\n",
    "Final.take(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}